{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import tensorflow \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU, Bidirectional, TimeDistributed, InputLayer\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import seaborn as sns\n",
    "from functools import reduce\n",
    "from phik import resources, report\n",
    "tensorflow.random.set_seed(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "close = pd.read_csv(\"data/close.csv\").add_suffix('_close').rename(columns={'Date_close':'Date'})\n",
    "adj_close = pd.read_csv(\"data/adj_close.csv\").add_suffix('_adj_close').rename(columns={'Date_adj_close':'Date'})\n",
    "high = pd.read_csv(\"data/high.csv\").add_suffix('_high').rename(columns={'Date_high':'Date'})\n",
    "low = pd.read_csv(\"data/low.csv\").add_suffix('_low').rename(columns={'Date_low':'Date'})\n",
    "open1 = pd.read_csv(\"data/open.csv\").add_suffix('_open').rename(columns={'Date_open':'Date'})\n",
    "volume = pd.read_csv(\"data/volume.csv\").add_suffix('_vol').rename(columns={'Date_vol':'Date'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing and concactenating the data to desired format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [close, adj_close, high, low, open1, volume]\n",
    "df_merged = reduce(lambda  left,right: pd.merge(left,right,on=['Date'],\n",
    "                                            how='outer'), dfs)\n",
    "\n",
    "def get_coin(coin,df = df_merged):\n",
    "    re = '^' + coin \n",
    "    new = df.filter(regex=re,axis=1)\n",
    "    new.columns  = new.columns.str.lstrip(coin + '_').dropna()\n",
    "    new['Date'] = df.loc[:,'Date']\n",
    "    new['Date'] = pd.to_datetime(new['Date'])\n",
    "    new = new.dropna()\n",
    "    new.name = coin\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#original dataset parsing \n",
    "ADA = get_coin('ADA-USD')\n",
    "ATOM = get_coin('ATOM-USD')\n",
    "AVAX = get_coin('AVAX-USD')\n",
    "AXS = get_coin('AXS-USD') \n",
    "BTC = get_coin('BTC-USD') \n",
    "ETH = get_coin('ETH-USD')\n",
    "LINK = get_coin('LINK-USD')\n",
    "LUNA1 = get_coin('LUNA1-USD') \n",
    "MATIC = get_coin('MATIC-USD') \n",
    "SOL = get_coin('SOL-USD') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "AVAX=AVAX.loc[~((AVAX['Date'] == '2020-07-13') | (AVAX['Date'] == '2020-07-14'))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary functions to run our model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_X_y(df, past_days,future_days= 1):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(past_days, len(df) - future_days +1):\n",
    "        X.append(df[i - past_days:i, 0:df.shape[1]])\n",
    "        y.append(df[i + future_days - 1:i + future_days, 0])\n",
    "\n",
    "    X, y = np.array(X), np.array(y)\n",
    "    \n",
    "    #print('X shape == {}.'.format(X.shape))\n",
    "    #print('y shape == {}.'.format(y.shape))\n",
    "    return X,y\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_sequential_model(model_name,trainX,trainY, metric):\n",
    "    model = Sequential()\n",
    "    if model_name == 'LSTM':\n",
    "        model.add(LSTM(64, activation='relu', input_shape=(trainX.shape[1], trainX.shape[2]), return_sequences=True))\n",
    "        model.add(LSTM(32, activation='relu', return_sequences=False))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(trainY.shape[1]))\n",
    "    else:\n",
    "        model.add(GRU(64, activation='relu', input_shape=(trainX.shape[1], trainX.shape[2]), return_sequences=True))\n",
    "        model.add(GRU(32, activation='relu', return_sequences=False))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(trainY.shape[1]))\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mse',metrics = metric)\n",
    "    #model.build(input_shape=(None,trainX.shape[1], trainX.shape[2]))\n",
    "\n",
    "    #model.summary()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_fit_model( model_name, df):\n",
    "    #split into train and test\n",
    "    train_size = int(len(df)*0.85)\n",
    "    test_size = len(df) - train_size\n",
    "    train, test = df[0:train_size,:],df[train_size:len(df)]\n",
    "    #define input shape\n",
    "    trainX,trainY = df_to_X_y(train,20)\n",
    "    testX,testY = df_to_X_y(test,20)\n",
    "        \n",
    "    #define metrics to use in our model \n",
    "    metric = ['mse','mae',RootMeanSquaredError()]\n",
    "    model = choose_sequential_model(model_name,trainX,trainY,metric)\n",
    "    history = model.fit(trainX, trainY, epochs=600, batch_size=32, validation_data=(testX,testY), verbose=0)\n",
    "    return trainX, trainY, testX, testY, history,model\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(history):\n",
    "    plt.plot(history.history['val_root_mean_squared_error'], label='Val RMSE')\n",
    "    plt.plot(history.history['val_mae'], label='Val MAE')\n",
    "    plt.plot(history.history['val_mse'], label='Val MSE')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_calc(dic,metric):\n",
    "    filtered_vals = dic.get(metric)\n",
    "    average = sum(filtered_vals) / len(filtered_vals)\n",
    "    print(metric + ' ----> ' + str(average))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final function, plots and final output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def coin_predictions(coin, model_n, multivariable='IND', metric_details=0):\n",
    "    tensorflow.random.set_seed(15)\n",
    "    indicators = ['rsi','macd']\n",
    "    #if multivariable is set to 0, the function makes the predictions based on only the closing price variable\n",
    "    if multivariable == 'CLOSE':\n",
    "        df_training = coin.close.to_frame()\n",
    "\n",
    "    #if multivariable is set to 1, the function makes the predictions based only on the OHLC variables\n",
    "    elif multivariable == 'OHLC' and (indicators not in coin.columns.to_list()):\n",
    "        cols = coin.drop(['adj_close','vol', 'Date'], axis = 1).columns\n",
    "        df_training = coin[cols].astype(float)\n",
    "\n",
    "    #if multivariable is set to 2, the function makes the predictions based on  the financial indicator variables\n",
    "    elif multivariable == 'IND':\n",
    "       cols = coin.drop(['adj_close','vol', 'Date','high', 'low', 'open'], axis = 1).columns\n",
    "       df_training = coin[cols].astype(float)\n",
    "    \n",
    "    #else, predictions are based on only financial indicator variables  + OHLC\n",
    "    else:\n",
    "        cols = coin.drop(['adj_close','vol', 'Date'], axis = 1).columns\n",
    "        df_training = coin[cols].astype(float)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #slice only the last 365 days to use as prediction data\n",
    "    df_training = df_training.tail(365)\n",
    "    \n",
    "    #scale the training dataset\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler = scaler.fit(df_training)\n",
    "    df_training_scaled = scaler.transform(df_training)\n",
    "\n",
    "    #run and fit our prediction model, where the model is stored in the variable model and the fit is stored in the history variable\n",
    "    trainX, trainY, testX, testY, history, model = run_fit_model(model_n, df_training_scaled)\n",
    "\n",
    "    #h_to_plot = history\n",
    "\n",
    "    #if metric_details is set to 1, details about the performance metrics are shown and plotted \n",
    "    if metric_details == 1:\n",
    "        itermetrics = ['val_root_mean_squared_error','val_mae','val_mse']\n",
    "        for i in itermetrics:\n",
    "            mean_calc(history.history,i)\n",
    "    \n",
    "        plot_metrics(history)\n",
    "\n",
    "    #generate date range for predtiction using the 20th previous predictions to predict the next day  \n",
    "    train_dates = pd.to_datetime(coin.Date.tail(365))\n",
    "    n_days_for_prediction = 20\n",
    "    predict_period_dates = pd.date_range(list(train_dates)[-19], periods=n_days_for_prediction).tolist()\n",
    "\n",
    "    forecast_dates = []\n",
    "    for time_i in predict_period_dates:\n",
    "        forecast_dates.append(time_i.date())\n",
    "\n",
    "    #make the prediction \n",
    "    prediction = model.predict(testX[-n_days_for_prediction:])\n",
    "    #inverse scale the obtained results\n",
    "    prediction_copies = np.repeat(prediction, df_training.shape[1], axis=-1)\n",
    "    \n",
    "    #create a new dataframe with the prediction\n",
    "    if multivariable == 'CLOSE':\n",
    "        y_pred_future = scaler.inverse_transform(prediction_copies)[:,0]\n",
    "        df_forecast = pd.DataFrame({'Date':np.array(forecast_dates), 'close':y_pred_future})\n",
    "\n",
    "        \n",
    "\n",
    "    elif multivariable == 'OHLC':\n",
    "        y_pred_future = scaler.inverse_transform(prediction_copies)\n",
    "        df_forecast = pd.DataFrame({'Date':np.array(forecast_dates), 'close':y_pred_future[:,0],  'high':y_pred_future[:,1],  'low':y_pred_future[:,2],  'open':y_pred_future[:,3]})\n",
    "\n",
    "        \n",
    "    \n",
    "    elif multivariable == 'IND':\n",
    "        y_pred_future = scaler.inverse_transform(prediction_copies)\n",
    "        df_forecast = pd.DataFrame({'Date':np.array(forecast_dates), 'close':y_pred_future[:,0],  'rsi':y_pred_future[:,1],  'macd':y_pred_future[:,2]})\n",
    "        \n",
    "   \n",
    "\n",
    "    else:\n",
    "        y_pred_future = scaler.inverse_transform(prediction_copies)\n",
    "        df_forecast = pd.DataFrame({'Date':np.array(forecast_dates), 'close':y_pred_future[:,0],  'high':y_pred_future[:,1],  'low':y_pred_future[:,2],  'open':y_pred_future[:,3]\n",
    "        , 'rsi':y_pred_future[:,4], 'macd':y_pred_future[:,5]})\n",
    "\n",
    "    \n",
    "    df_forecast['Date']=pd.to_datetime(df_forecast['Date'])\n",
    "    \n",
    "    original = coin.tail(365)\n",
    "    original['Date']=pd.to_datetime(original['Date'])\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "    return df_forecast, original \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADA_I_1 = pd.read_csv(\"data_updated/ADA_IND.csv\")\n",
    "ADA_I_1.name = 'ADA-USD'\n",
    "ATOM_I_1 = pd.read_csv(\"data_updated/ATOM_IND.csv\")\n",
    "ATOM_I_1.name = 'ATOM-USD'\n",
    "AVAX_I_1 = pd.read_csv(\"data_updated/AVAX_IND.csv\")\n",
    "AVAX_I_1.name = 'AVAX-USD'\n",
    "AXS_I_1 = pd.read_csv(\"data_updated/AXS_IND.csv\")\n",
    "AXS_I_1.name = 'AXS-USD'\n",
    "BTC_I_1 = pd.read_csv(\"data_updated/BTC_IND.csv\")\n",
    "BTC_I_1.name = 'BTC-USD'\n",
    "ETH_I_1 = pd.read_csv(\"data_updated/ETH_IND.csv\")\n",
    "ETH_I_1.name = 'ETH-USD'\n",
    "LINK_I_1 = pd.read_csv(\"data_updated/LINK_IND.csv\")\n",
    "LINK_I_1.name = 'LINK-USD'\n",
    "LUNA1_I_1 = pd.read_csv(\"data_updated/LUNA1_IND.csv\")\n",
    "LUNA1_I_1.name = 'LUNA1-USD'\n",
    "MATIC_I_1 = pd.read_csv(\"data_updated/MATIC_IND.csv\")\n",
    "MATIC_I_1.name = 'MATIC-USD'\n",
    "SOL_I_1 = pd.read_csv(\"data_updated/SOL_IND.csv\")\n",
    "SOL_I_1.name = 'SOL-USD'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeatRows(d, n=1):\n",
    "    return pd.concat([d]*n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_output(n_days):  \n",
    "    coins_I = [ADA_I_1,ATOM_I_1,AVAX_I_1,AXS_I_1, BTC_I_1,ETH_I_1, LINK_I_1,LUNA1_I_1, MATIC_I_1, SOL_I_1 ]\n",
    "    for i in range(n_days):\n",
    "        for coin in range(len(coins_I)):\n",
    "            coin_name = coins_I[coin].name\n",
    "            forecast, original = coin_predictions(coins_I[coin],'GRU')\n",
    "            new_data = pd.concat([original,repeatRows(original[-1:], 1)])\n",
    "            new_data.iloc[-1,[0,1,7,8]] = [forecast['Date'].iloc[-1],forecast['close'].iloc[-1],forecast['rsi'].iloc[-1],forecast['macd'].iloc[-1]]\n",
    "            coins_I[coin] = new_data\n",
    "            print('- ' + coin_name + ': ', forecast['close'].iloc[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- ADA-USD:  0.8217862\n",
      "- ATOM-USD:  16.769114\n",
      "- AVAX-USD:  55.15369\n"
     ]
    }
   ],
   "source": [
    "final_output(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dabe2e9bff96db92a91b2d3763a3bbd0cf4fa39a7bf913bd4d06c148aca0ebc0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tensorflow_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
